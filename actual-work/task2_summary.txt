
TASK 2: DATA PREPROCESSING STRATEGY AND JUSTIFICATION

Missing Value Handling (Lines 81-120):
Analysis of the preprocessed data revealed no missing values across all three 
splits. Therefore, no imputation strategy was required.

Feature Scaling (Lines 121-180):
We applied StandardScaler normalization to all 88 audio features. 
This preprocessing step is critical for several reasons:

1. SVM algorithms are sensitive to feature scales and require normalized inputs
2. Features with large magnitudes (e.g., frequency values) would otherwise 
   dominate the learning process
3. Standardization (zero mean, unit variance) improves convergence for 
   optimization-based algorithms

The scaler was fit exclusively on the training set (Lines 127-128) to prevent 
data leakage. The same transformation was then applied to validation and test 
sets using the training statistics. Post-scaling verification confirmed that 
scaled features have approximately zero mean and unit standard deviation.

Class Imbalance Handling (Lines 181-240):
Both target variables exhibit severe class imbalance:
- target_human: 8.90x imbalance ratio
- target_asv: 27.29x imbalance ratio (particularly severe)

The target_asv label is especially problematic, with the "sheep" class 
representing 80.4% of training samples. 
A naive classifier predicting only "sheep" would achieve high accuracy but 
poor performance on minority classes.

To address this imbalance, we will use the class_weight='balanced' parameter 
in our classification algorithms (implemented in Step 4). This approach 
automatically weights classes inversely proportional to their frequencies, 
ensuring the model learns to identify minority classes without discarding 
valuable majority class data or introducing synthetic samples.

Alternative approaches (SMOTE, undersampling) were considered but rejected:
- SMOTE risks overfitting on synthetic minority samples
- Undersampling would discard 2550 
  valuable majority class samples

Feature Selection:
Initial experiments use all 88 features. Feature selection 
techniques (e.g., mutual information, recursive feature elimination) were not 
applied in the baseline to establish full feature set performance. This could 
be explored in hyperparameter tuning if initial results suggest overfitting.
